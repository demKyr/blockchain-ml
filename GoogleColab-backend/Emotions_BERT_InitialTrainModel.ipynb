{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l2812uUh8W6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vnCsM_7o290l"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wJGj2ZCae-ro"
      },
      "outputs": [],
      "source": [
        "model_save_name = 'BERTModel-Emotions'\n",
        "path = F\"/content/gdrive/MyDrive/Colab Notebooks/ml-blockchain/savedModels/{model_save_name}\" \n",
        "testSetPath = F\"/content/gdrive/MyDrive/Colab Notebooks/ml-blockchain/savedModels/{model_save_name}-test.csv\"\n",
        "validationSetPath = F\"/content/gdrive/MyDrive/Colab Notebooks/ml-blockchain/savedModels/{model_save_name}-validation.csv\"\n",
        "labels = ['Sadness', 'Anger', 'Love', 'Surprise', 'Fear', 'Happiness']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rKBddi32Qgr"
      },
      "source": [
        "## Install Transformers library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvX417-Az9kU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woQ26DuF2bJQ"
      },
      "source": [
        "## Load the BERT Classifier and Tokenizer along with Input modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYrk0zdB2C6x"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XocbxBF2J6H"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfeppyT-TNGt"
      },
      "source": [
        "## Download Kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndiIJLvbTWaW"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/gdrive/MyDrive/Colab\\ Notebooks/ml-blockchain/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d ishantjuyal/emotions-in-text\n",
        "! unzip emotions-in-text.zip"
      ],
      "metadata": {
        "id": "ecBVy4KwEW48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lf9OFWiA1oe8"
      },
      "outputs": [],
      "source": [
        "csv_path = '/content/Emotion_final.csv'\n",
        "dataset_file = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ865Tr619w_"
      },
      "outputs": [],
      "source": [
        "dataset_file.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_file.Emotion.unique())"
      ],
      "metadata": {
        "id": "0fC8SUD_BS7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_file.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
        "dataset_file['LABEL_COLUMN'] = dataset_file['LABEL_COLUMN'].replace('sadness', 0)\n",
        "dataset_file['LABEL_COLUMN'] = dataset_file['LABEL_COLUMN'].replace('anger', 1)\n",
        "dataset_file['LABEL_COLUMN'] = dataset_file['LABEL_COLUMN'].replace('love', 2)\n",
        "dataset_file['LABEL_COLUMN'] = dataset_file['LABEL_COLUMN'].replace('surprise', 3)\n",
        "dataset_file['LABEL_COLUMN'] = dataset_file['LABEL_COLUMN'].replace('fear', 4)\n",
        "dataset_file['LABEL_COLUMN'] = dataset_file['LABEL_COLUMN'].replace('happy', 5)\n",
        "dataset_file"
      ],
      "metadata": {
        "id": "EiBKipA4GGsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P7Jy3w73cLA"
      },
      "source": [
        "## Split dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alN5hOVd3bWJ"
      },
      "outputs": [],
      "source": [
        "train, test_and_validatition = train_test_split(dataset_file, test_size=0.2, random_state=77)\n",
        "test, validation = train_test_split(test_and_validatition, test_size=0.5, random_state=77)\n",
        "print(len(train),len(test),len(validation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItMS_Zs4F17K"
      },
      "source": [
        "## Save test and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D58GSBUiETtJ"
      },
      "outputs": [],
      "source": [
        "with open(testSetPath, 'w', encoding = 'utf-8-sig') as f:\n",
        "  test.to_csv(f)\n",
        "\n",
        "with open(validationSetPath, 'w', encoding = 'utf-8-sig') as f:\n",
        "  validation.to_csv(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db9cZAYo8IY2"
      },
      "source": [
        "## Create input sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CEcDQVmF8D1Y"
      },
      "outputs": [],
      "source": [
        "def convert_data_to_examples_single(inputDataset, DATA_COLUMN, LABEL_COLUMN): \n",
        "  train_InputExamples = inputDataset.apply(lambda x: InputExample(guid=None, \n",
        "                                                          text_a = x[DATA_COLUMN], \n",
        "                                                          text_b = None,\n",
        "                                                          label = x[LABEL_COLUMN]), axis = 1)  \n",
        "  return train_InputExamples\n",
        "\n",
        "\n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = [] \n",
        "\n",
        "    for e in examples:\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length, \n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True, \n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
        "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aA7L0O-b6FZz"
      },
      "outputs": [],
      "source": [
        "DATA_COLUMN = 'DATA_COLUMN'\n",
        "LABEL_COLUMN = 'LABEL_COLUMN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY0NyucTqqkG"
      },
      "outputs": [],
      "source": [
        "test_inputExamples = convert_data_to_examples_single(test, DATA_COLUMN, LABEL_COLUMN)\n",
        "test_data = convert_examples_to_tf_dataset(list(test_inputExamples), tokenizer)\n",
        "test_data = test_data.batch(32)\n",
        "\n",
        "validation_InputExamples = convert_data_to_examples_single(validation, DATA_COLUMN, LABEL_COLUMN)\n",
        "validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
        "validation_data = validation_data.batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks6aICua3HgH"
      },
      "source": [
        "## Configure the Loaded BERT model and Train for Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFUU-8Sk-Ib0"
      },
      "outputs": [],
      "source": [
        "train_InputExamples  = convert_data_to_examples_single(train, DATA_COLUMN, LABEL_COLUMN)\n",
        "train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
        "train_data = train_data.shuffle(100).batch(32).repeat(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWs-hscH-Tcl"
      },
      "outputs": [],
      "source": [
        "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=3e-5, epsilon=1e-08), \n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "\n",
        "model.fit(train_data, epochs=2, validation_data=validation_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQNYjUt26i7m"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model"
      ],
      "metadata": {
        "id": "p3S3337Z0taF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sgGIyowe6rXA"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAnMQm_XNFYH"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joCpy8bbNELe"
      },
      "outputs": [],
      "source": [
        "loaded_model = TFBertForSequenceClassification.from_pretrained(path, local_files_only=True)\n",
        "loaded_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nVRl9Jb-PNx"
      },
      "source": [
        "## Make Predictions with the Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-VkUzBzx-i7l"
      },
      "outputs": [],
      "source": [
        "pred_sentences = ['I am scared of the dark',\n",
        "                  'I want to spend the rest of my life with you',\n",
        "                  'He was filled with joy when he opened his present',\n",
        "                  'She was devastated after the death of her husband']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdrlZK_G-lcK"
      },
      "outputs": [],
      "source": [
        "tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
        "tf_outputs = loaded_model(tf_batch)\n",
        "tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
        "label = tf.argmax(tf_predictions, axis=1)\n",
        "label = label.numpy()\n",
        "for i in range(len(pred_sentences)):\n",
        "  print(pred_sentences[i], \": \\n\", labels[label[i]])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cfeppyT-TNGt",
        "db9cZAYo8IY2"
      ],
      "name": "Emotions_BERT_InitialTrainModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}