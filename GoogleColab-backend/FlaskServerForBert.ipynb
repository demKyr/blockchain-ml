{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19204,"status":"ok","timestamp":1654672754206,"user":{"displayName":"Demetris Kyriacou","userId":"16887261084371161080"},"user_tz":-180},"id":"dY3DuQU_IbY3","outputId":"36cc552c-43eb-47e9-ec33-c230b5309cac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20181,"status":"ok","timestamp":1654672774384,"user":{"displayName":"Demetris Kyriacou","userId":"16887261084371161080"},"user_tz":-180},"id":"KISCdlM5Ih6W","outputId":"e7c2fb9e-e3fe-4355-deff-b816d561d032"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 27.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 63.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 65.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (1.1.4)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask) (7.1.2)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask) (1.0.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask) (2.0.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting flask-ngrok\n","  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (2.23.0)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask-ngrok) (1.1.4)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.3)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask-ngrok) (2.0.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask-ngrok) (3.0.4)\n","Installing collected packages: flask-ngrok\n","Successfully installed flask-ngrok-0.0.25\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting flask-cors\n","  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n","Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.1.4)\n","Requirement already satisfied: Six in /usr/local/lib/python3.7/dist-packages (from flask-cors) (1.15.0)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.0.1)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (2.11.3)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (1.1.0)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.9->flask-cors) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.9->flask-cors) (2.0.1)\n","Installing collected packages: flask-cors\n","Successfully installed flask-cors-3.0.10\n"]}],"source":["! pip install transformers\n","! pip install flask\n","! pip install flask-ngrok\n","! pip install -U flask-cors"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1654672774385,"user":{"displayName":"Demetris Kyriacou","userId":"16887261084371161080"},"user_tz":-180},"id":"z0hOrGYFIuRP","outputId":"dc8f82d9-34ab-4b3d-c6fc-bb60e23abc43"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: ngrok: command not found\n"]}],"source":["! ngrok authtoken 29rA6VvxT2voCttRs1SXLztpfoK_3W93BncBB2LHHfXxVbfUj"]},{"cell_type":"code","source":["modelNames = [\"BERTModel-Twitter\", \"BERTModel-IMDB\", \"BERTModel-Emotions\"]\n","labels = [['Positive','Negative','Neutral'], ['Negative','Positive'], ['Sadness', 'Anger', 'Love', 'Surprise', 'Fear', 'Happiness']]"],"metadata":{"id":"XndQhNiIEJb_","executionInfo":{"status":"ok","timestamp":1654672774386,"user_tz":-180,"elapsed":12,"user":{"displayName":"Demetris Kyriacou","userId":"16887261084371161080"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bxvkr5cFIklI","outputId":"928396cf-e32c-4ef2-eddf-b24cd360703b","executionInfo":{"status":"ok","timestamp":1654675821924,"user_tz":-180,"elapsed":626678,"user":{"displayName":"Demetris Kyriacou","userId":"16887261084371161080"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["0 BERTModel-Twitter\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at /content/gdrive/MyDrive/Colab_Notebooks/savedModels/BERTModel-Twitter were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/gdrive/MyDrive/Colab_Notebooks/savedModels/BERTModel-Twitter.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["1 BERTModel-IMDB\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at /content/gdrive/MyDrive/Colab_Notebooks/savedModels/BERTModel-IMDB were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/gdrive/MyDrive/Colab_Notebooks/savedModels/BERTModel-IMDB.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["2 BERTModel-Emotions\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at /content/gdrive/MyDrive/Colab_Notebooks/savedModels/BERTModel-Emotions were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/gdrive/MyDrive/Colab_Notebooks/savedModels/BERTModel-Emotions.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Server Running🚀\n"," * Serving Flask app \"__main__\" (lazy loading)\n"," * Environment: production\n","\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n","\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":[" * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":[" * Running on http://0211-35-245-112-159.ngrok.io\n"," * Traffic stats available on http://127.0.0.1:4040\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [08/Jun/2022 08:00:44] \"\u001b[37mPOST /test?model=0 HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [08/Jun/2022 08:00:51] \"\u001b[37mPOST /test?model=1 HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [08/Jun/2022 08:00:59] \"\u001b[37mPOST /test?model=2 HTTP/1.1\u001b[0m\" 200 -\n","127.0.0.1 - - [08/Jun/2022 08:01:03] \"\u001b[37mPOST /test?model=2 HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["Starting Training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","2/2 [==============================] - 46s 26s/step - loss: 0.7385 - accuracy: 0.8333 - val_loss: 0.8666 - val_accuracy: 0.7850\n","Epoch 2/2\n","2/2 [==============================] - 24s 24s/step - loss: 0.0411 - accuracy: 1.0000 - val_loss: 0.8732 - val_accuracy: 0.7887\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [08/Jun/2022 08:02:26] \"\u001b[37mPOST /train?model=0 HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["Starting Evaluation\n","86/86 [==============================] - 27s 280ms/step - loss: 0.9544 - accuracy: 0.7624\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [08/Jun/2022 08:03:11] \"\u001b[37mGET /evaluate?model=0 HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["Starting Training\n","Epoch 1/2\n","2/2 [==============================] - 56s 37s/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5739 - val_accuracy: 0.8867\n","Epoch 2/2\n","2/2 [==============================] - 35s 35s/step - loss: 8.8433e-04 - accuracy: 1.0000 - val_loss: 0.5974 - val_accuracy: 0.8870\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [08/Jun/2022 08:05:49] \"\u001b[37mPOST /train?model=1 HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["Starting Evaluation\n","125/125 [==============================] - 37s 278ms/step - loss: 0.5925 - accuracy: 0.8867\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [08/Jun/2022 08:06:50] \"\u001b[37mGET /evaluate?model=1 HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["Starting Training\n","Epoch 1/2\n","2/2 [==============================] - 39s 21s/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.2530 - val_accuracy: 0.9245\n","Epoch 2/2\n","2/2 [==============================] - 19s 19s/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.2612 - val_accuracy: 0.9245\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [08/Jun/2022 08:08:29] \"\u001b[37mPOST /train?model=2 HTTP/1.1\u001b[0m\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["Starting Evaluation\n","68/68 [==============================] - 21s 275ms/step - loss: 0.2756 - accuracy: 0.9245\n"]},{"output_type":"stream","name":"stderr","text":["127.0.0.1 - - [08/Jun/2022 08:08:53] \"\u001b[37mGET /evaluate?model=2 HTTP/1.1\u001b[0m\" 200 -\n"]}],"source":["from flask import Flask, jsonify, request, Response\n","from flask_cors import CORS, cross_origin\n","from flask_ngrok import run_with_ngrok\n","\n","import json\n","\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from transformers import InputExample, InputFeatures\n","\n","\n","# model_save_name = 'BERTModel'\n","# path = F\"./NeuralNetwork/{model_save_name}\" \n","# model_save_name = 'BERTModel'\n","# path = F\"/content/gdrive/MyDrive/Colab_Notebooks/savedModels/{model_save_name}\" \n","# testSetPath = F\"/content/gdrive/MyDrive/Colab_Notebooks/savedModels/{model_save_name}-test.csv\"\n","# validationSetPath = F\"/content/gdrive/MyDrive/Colab_Notebooks/savedModels/{model_save_name}-validation.csv\"\n"," \n","\n","def loadModel(model_save_name):\n","    path = F\"/content/gdrive/MyDrive/Colab_Notebooks/savedModels/{model_save_name}\" \n","    testSetPath = F\"/content/gdrive/MyDrive/Colab_Notebooks/savedModels/{model_save_name}-test.csv\"\n","    validationSetPath = F\"/content/gdrive/MyDrive/Colab_Notebooks/savedModels/{model_save_name}-validation.csv\"\n","\n","    # ## Load the BERT Classifier and Tokenizer along with Input modules\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","    # ## Load CSV, test and validation datasets\n","    # test = pd.read_csv('./NeuralNetwork/test.csv')\n","    # validation = pd.read_csv('./NeuralNetwork/validation.csv')\n","    test = pd.read_csv(testSetPath)\n","    validation = pd.read_csv(validationSetPath)\n","    test = test.iloc[: , 1:]\n","    validation = validation.iloc[: , 1:]\n","\n","    loaded_model = TFBertForSequenceClassification.from_pretrained(path, local_files_only=True)\n","\n","    return loaded_model,tokenizer,test,validation\n","\n","def convert_data_to_examples_single(inputDataset, DATA_COLUMN, LABEL_COLUMN): \n","    train_InputExamples = inputDataset.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n","                                                        text_a = x[DATA_COLUMN], \n","                                                        text_b = None,\n","                                                        label = x[LABEL_COLUMN]), axis = 1)  \n","    return train_InputExamples\n","\n","def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n","    features = [] # -> will hold InputFeatures to be converted later\n","\n","    for e in examples:\n","        # Documentation is really strong for this method, so please take a look at it\n","        input_dict = tokenizer.encode_plus(\n","            e.text_a,\n","            add_special_tokens=True,\n","            max_length=max_length, # truncates if len(s) > max_length\n","            return_token_type_ids=True,\n","            return_attention_mask=True,\n","            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n","            truncation=True\n","        )\n","\n","        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n","            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n","            )\n","        )\n","\n","    def gen():\n","        for f in features:\n","            yield (\n","                {\n","                    \"input_ids\": f.input_ids,\n","                    \"attention_mask\": f.attention_mask,\n","                    \"token_type_ids\": f.token_type_ids,\n","                },\n","                f.label,\n","            )\n","\n","    return tf.data.Dataset.from_generator(\n","        gen,\n","        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n","        (\n","            {\n","                \"input_ids\": tf.TensorShape([None]),\n","                \"attention_mask\": tf.TensorShape([None]),\n","                \"token_type_ids\": tf.TensorShape([None]),\n","            },\n","            tf.TensorShape([]),\n","        ),\n","    )\n","\n","def trainModel(loadedModel,tokenizer,validation,train,model_save_name):\n","\n","    path = F\"/content/gdrive/MyDrive/Colab_Notebooks/savedModels/{model_save_name}\" \n","\n","    DATA_COLUMN = 'DATA_COLUMN'\n","    LABEL_COLUMN = 'LABEL_COLUMN'\n","\n","    validation_InputExamples = convert_data_to_examples_single(validation, DATA_COLUMN, LABEL_COLUMN)\n","    validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n","    validation_data = validation_data.batch(32)\n","\n","    train_InputExamples = convert_data_to_examples_single(train, DATA_COLUMN, LABEL_COLUMN)\n","    train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n","    train_data = train_data.batch(32).repeat(2)\n","\n","    # ## Retrain BERT model\n","    loadedModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n","                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","                metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n","\n","    loadedModel.fit(train_data, epochs=2, validation_data=validation_data)\n","\n","    # # model_save_name = 'BERTModel'\n","    # # path = F\"./NeuralNetwork/{model_save_name}\" \n","    # model_save_name = 'BERTModel1(2022-05-21 10:34)'\n","    # path = F\"/content/gdrive/MyDrive/Colab_Notebooks/savedModels/{model_save_name}\" \n","    loadedModel.save_pretrained(path)\n","\n","    return loadedModel\n","\n","def evaluateModel(loadedModel,tokenizer,test):\n","    DATA_COLUMN = 'DATA_COLUMN'\n","    LABEL_COLUMN = 'LABEL_COLUMN'\n","\n","    test_inputExamples = convert_data_to_examples_single(test, DATA_COLUMN, LABEL_COLUMN)\n","    test_data = convert_examples_to_tf_dataset(list(test_inputExamples), tokenizer)\n","    test_data = test_data.batch(32)\n","    \n","    loadedModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n","                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","                metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n","\n","    evaluation = loadedModel.evaluate(test_data)\n","\n","    return (evaluation)\n","\n","def testModel(loadedModel,tokenizer,pred_sentences,labels):\n","\n","    tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n","    tf_outputs = loadedModel(tf_batch)\n","    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n","    # labels = ['Positive','Negative','Neutral']\n","    label = tf.argmax(tf_predictions, axis=1)\n","    label = label.numpy()\n","    return labels[label[0]]\n","\n","###################################################################################################################\n","\n","app = Flask(__name__)\n","\n","\n","run_with_ngrok(app)  \n","CORS(app)\n","\n","loadedModel = [None] * len(modelNames)\n","tokenizer = [None] * len(modelNames)\n","test = [None] * len(modelNames)\n","validation = [None] * len(modelNames)\n","\n","for id,modelName in enumerate(modelNames):\n","  print(id,modelName)\n","  loadedModel[id],tokenizer[id],test[id],validation[id] = loadModel(modelName)\n","\n","\n","@app.route('/train', methods=['POST'])\n","def trainFun():\n","    print('Starting Training')\n","    \n","    modelId = int(request.args.get('model'))\n","\n","    # trainSetInput = request.json\n","    trainSetInput = json.loads(request.data.decode('utf-8'))\n","\n","    trainDF = pd.DataFrame(trainSetInput)\n","    del trainDF['id']\n","    del trainDF['proposedLbl']\n","    del trainDF['goodData']\n","    trainDF.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n","    trainModel(loadedModel[modelId],tokenizer[modelId],validation[modelId],trainDF,modelNames[modelId])\n","    \n","    # response = Response(status=200)\n","    response = jsonify(status=\"ok\")\n","    return response\n","\n","\n","@app.route('/evaluate')\n","def evaluateFun():\n","    print('Starting Evaluation')\n","\n","    modelId = int(request.args.get('model'))\n","\n","    eval = evaluateModel(loadedModel[modelId],tokenizer[modelId],test[modelId])\n","    [loss,acc] = eval\n","\n","    response = jsonify(loss=loss,acc=acc)\n","    return response\n","\n","\n","@app.route('/test', methods=['POST'])\n","def testFun():\n","\n","    modelId = int(request.args.get('model'))\n","\n","    dataIn = json.loads(request.data.decode('utf-8'))\n","    pred_sent = dataIn['caption']\n","\n","    # pred_sent = request.json['caption']\n","    pred = testModel(loadedModel[modelId],tokenizer[modelId],pred_sent,labels[modelId])\n","\n","    response = jsonify(prediction=pred)\n","    return response\n","\n","\n","\n","\n","if (__name__ == \"__main__\"):\n","    print('Server Running🚀')\n","    app.run()\n","\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Αντίγραφο του FlaskServerForBert.ipynb","provenance":[],"authorship_tag":"ABX9TyNyPumv9yoJnlI4F/Tcoti4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}