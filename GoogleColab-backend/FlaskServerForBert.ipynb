{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY3DuQU_IbY3"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KISCdlM5Ih6W"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n",
        "! pip install flask\n",
        "! pip install flask-ngrok\n",
        "! pip install -U flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XndQhNiIEJb_"
      },
      "outputs": [],
      "source": [
        "modelNames = [\"BERTModel-Twitter\", \"BERTModel-IMDB\", \"BERTModel-Emotions\"]\n",
        "labels = [['Positive','Negative','Neutral'], ['Negative','Positive'], ['Sadness', 'Anger', 'Love', 'Surprise', 'Fear', 'Happiness']]\n",
        "DATA_COLUMN = 'DATA_COLUMN'\n",
        "LABEL_COLUMN = 'LABEL_COLUMN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bxvkr5cFIklI"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, jsonify, request, Response\n",
        "from flask_cors import CORS\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from transformers import InputExample, InputFeatures\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Function that loads a saved model with its test and validation sets\n",
        "def loadModel(model_save_name):\n",
        "    path = F\"/content/gdrive/MyDrive/Colab Notebooks/ml-blockchain/savedModels/{model_save_name}\" \n",
        "    testSetPath = F\"/content/gdrive/MyDrive/Colab Notebooks/ml-blockchain/savedModels/{model_save_name}-test.csv\"\n",
        "    validationSetPath = F\"/content/gdrive/MyDrive/Colab Notebooks/ml-blockchain/savedModels/{model_save_name}-validation.csv\"\n",
        "\n",
        "    test = pd.read_csv(testSetPath)\n",
        "    validation = pd.read_csv(validationSetPath)\n",
        "    test = test.iloc[: , 1:]\n",
        "    validation = validation.iloc[: , 1:]\n",
        "\n",
        "    loaded_model = TFBertForSequenceClassification.from_pretrained(path, local_files_only=True)\n",
        "    return loaded_model,tokenizer,test,validation\n",
        "\n",
        "\n",
        "# Functions that preprocess input data before training using tokenizer\n",
        "def convert_data_to_examples_single(inputDataset, DATA_COLUMN, LABEL_COLUMN): \n",
        "    train_InputExamples = inputDataset.apply(lambda x: InputExample(guid=None,\n",
        "                                                        text_a = x[DATA_COLUMN], \n",
        "                                                        text_b = None,\n",
        "                                                        label = x[LABEL_COLUMN]), axis = 1)  \n",
        "    return train_InputExamples\n",
        "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
        "    features = []\n",
        "    for e in examples:\n",
        "        input_dict = tokenizer.encode_plus(\n",
        "            e.text_a,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_length, \n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            pad_to_max_length=True, \n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
        "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
        "        features.append(\n",
        "            InputFeatures(\n",
        "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
        "            )\n",
        "        )\n",
        "    def gen():\n",
        "        for f in features:\n",
        "            yield (\n",
        "                {\n",
        "                    \"input_ids\": f.input_ids,\n",
        "                    \"attention_mask\": f.attention_mask,\n",
        "                    \"token_type_ids\": f.token_type_ids,\n",
        "                },\n",
        "                f.label,\n",
        "            )\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        gen,\n",
        "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
        "        (\n",
        "            {\n",
        "                \"input_ids\": tf.TensorShape([None]),\n",
        "                \"attention_mask\": tf.TensorShape([None]),\n",
        "                \"token_type_ids\": tf.TensorShape([None]),\n",
        "            },\n",
        "            tf.TensorShape([]),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "# Function that retrains and replaces a loaded model using a new train set \n",
        "def trainModel(loadedModel,tokenizer,validation,train,model_save_name):\n",
        "    # pass validation set through tokenizer\n",
        "    validation_InputExamples = convert_data_to_examples_single(validation, DATA_COLUMN, LABEL_COLUMN)\n",
        "    validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
        "    validation_data = validation_data.batch(32)\n",
        "    # pass train set through tokenizer\n",
        "    train_InputExamples = convert_data_to_examples_single(train, DATA_COLUMN, LABEL_COLUMN)\n",
        "    train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
        "    train_data = train_data.batch(32).repeat(2)\n",
        "    # Retrain model\n",
        "    loadedModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "    loadedModel.fit(train_data, epochs=2, validation_data=validation_data)\n",
        "    # Replace model\n",
        "    path = F\"/content/gdrive/MyDrive/Colab Notebooks/ml-blockchain/savedModels/{model_save_name}\" \n",
        "    loadedModel.save_pretrained(path)\n",
        "    return loadedModel\n",
        "\n",
        "# Function that evaluates a model using its test set\n",
        "def evaluateModel(loadedModel,tokenizer,test):\n",
        "    # pass test set through tokenizer\n",
        "    test_inputExamples = convert_data_to_examples_single(test, DATA_COLUMN, LABEL_COLUMN)\n",
        "    test_data = convert_examples_to_tf_dataset(list(test_inputExamples), tokenizer)\n",
        "    test_data = test_data.batch(32)\n",
        "    # Evaluate model\n",
        "    loadedModel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
        "    evaluation = loadedModel.evaluate(test_data)\n",
        "    return (evaluation)\n",
        "\n",
        "# Fucntion that predicts a laber for a caption using a model\n",
        "def testModel(loadedModel,tokenizer,pred_sentences,labels):\n",
        "    # pass caption through tokenizer\n",
        "    tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
        "    # predict caption\n",
        "    tf_outputs = loadedModel(tf_batch)\n",
        "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
        "    label = tf.argmax(tf_predictions, axis=1)\n",
        "    label = label.numpy()\n",
        "    return labels[label[0]]\n",
        "\n",
        "###################################################################################################################\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app) # expose port in public URL\n",
        "CORS(app) # Disable CORS restrictions\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Download tokenizer\n",
        "# Create empty arrays to save models, test sets and validation sets\n",
        "loadedModel = [None] * len(modelNames)\n",
        "test = [None] * len(modelNames)\n",
        "validation = [None] * len(modelNames)\n",
        "\n",
        "# Load all saved models\n",
        "for id,modelName in enumerate(modelNames):\n",
        "  print(id,modelName)\n",
        "  loadedModel[id],tokenizer,test[id],validation[id] = loadModel(modelName)\n",
        "\n",
        "# /train endpoint: Retrain model\n",
        "@app.route('/train', methods=['POST'])\n",
        "def trainFun():\n",
        "    print('Starting Training')\n",
        "    modelId = int(request.args.get('model'))\n",
        "\n",
        "    # trainSetInput = request.json # used if request contains json file\n",
        "    trainSetInput = json.loads(request.data.decode('utf-8')) # used if request contains raw text\n",
        "\n",
        "    trainDF = pd.DataFrame(trainSetInput)\n",
        "    del trainDF['id']\n",
        "    del trainDF['proposedLbl']\n",
        "    del trainDF['goodData']\n",
        "    trainDF.columns = ['DATA_COLUMN', 'LABEL_COLUMN']\n",
        "    trainModel(loadedModel[modelId],tokenizer,validation[modelId],trainDF,modelNames[modelId])\n",
        "    \n",
        "    # response = Response(status=200)\n",
        "    response = jsonify(status=\"ok\")\n",
        "    return response\n",
        "\n",
        "\n",
        "# /evaluate endpoint: Evaluate model\n",
        "@app.route('/evaluate')\n",
        "def evaluateFun():\n",
        "    print('Starting Evaluation')\n",
        "    modelId = int(request.args.get('model'))\n",
        "\n",
        "    eval = evaluateModel(loadedModel[modelId],tokenizer,test[modelId])\n",
        "    [loss,acc] = eval\n",
        "\n",
        "    response = jsonify(loss=loss,acc=acc)\n",
        "    return response\n",
        "\n",
        "\n",
        "# /test endpoint: Predicts label for a caption\n",
        "@app.route('/test', methods=['POST'])\n",
        "def testFun():\n",
        "    modelId = int(request.args.get('model'))\n",
        "\n",
        "    dataIn = json.loads(request.data.decode('utf-8'))\n",
        "    pred_sent = dataIn['caption'] # used if request contains raw text\n",
        "    # pred_sent = request.json['caption'] # used if request contains json file\n",
        "\n",
        "    pred = testModel(loadedModel[modelId],tokenizer,pred_sent,labels[modelId])\n",
        "\n",
        "    response = jsonify(prediction=pred)\n",
        "    return response\n",
        "\n",
        "\n",
        "if (__name__ == \"__main__\"):\n",
        "    print('Server RunningðŸš€')\n",
        "    app.run()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FlaskServerForBert.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}